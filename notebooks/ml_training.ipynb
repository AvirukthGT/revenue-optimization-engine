{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7717d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Connect\n",
    "env_path = Path('..') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    warehouse=\"COMPUTE_WH\",\n",
    "    database=\"DYNAMIC_PRICING\",\n",
    "    schema=\"MARTS\"\n",
    ")\n",
    "\n",
    "# 2. THE AGGREGATION QUERY\n",
    "# I group by Date and Category to see \"Daily Demand\"\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    SALES_DATE,\n",
    "    CATEGORY_NAME,\n",
    "    WEATHER_CONDITION,\n",
    "    AVG(OUR_PRICE) as avg_price,\n",
    "    AVG(COMPETITOR_PRICE) as avg_competitor_price,\n",
    "    COUNT(ORDER_ID) as quantity_sold  -- <--- THIS IS THE TARGET\n",
    "FROM DYNAMIC_PRICING.MARTS.MART_FULL_SALES_LOG\n",
    "GROUP BY 1, 2, 3\n",
    "ORDER BY SALES_DATE DESC\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(query)\n",
    "data = cur.fetchall()\n",
    "cols = [desc[0] for desc in cur.description]\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "# Create \"Price Ratio\" (Am I cheaper or expensive?)\n",
    "df['PRICE_RATIO'] = df['AVG_PRICE'] / df['AVG_COMPETITOR_PRICE']\n",
    "\n",
    "# Extract \"Day of Week\" (0=Mon, 6=Sun)\n",
    "df['SALES_DATE'] = pd.to_datetime(df['SALES_DATE'])\n",
    "df['day_of_week'] = df['SALES_DATE'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "print(f\"Training Data Ready: {df.shape[0]} daily summaries.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834dc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor # You might need to pip install xgboost\n",
    "\n",
    "# 1. Define Features (X) and Target (y)\n",
    "X = df[['CATEGORY_NAME', 'WEATHER_CONDITION', 'AVG_PRICE', 'PRICE_RATIO', 'is_weekend']]\n",
    "y = df['QUANTITY_SOLD']\n",
    "\n",
    "# 2. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Define Preprocessing Steps\n",
    "# Categorical cols (Category, Weather) -> OneHotEncode\n",
    "# Numerical cols (Price, Ratio) -> Scale (Standardize)\n",
    "categorical_features = ['CATEGORY_NAME', 'WEATHER_CONDITION']\n",
    "numerical_features = ['AVG_PRICE', 'PRICE_RATIO', 'is_weekend']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 4. Create the Pipeline structure (Empty model for now)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', RandomForestRegressor())]) # Placeholder\n",
    "\n",
    "print(\"Pipeline constructed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"Hyperparameter Grid\"\n",
    "# This tells Python: \"Try all these combinations and tell me the winner\"\n",
    "param_grid = [\n",
    "    # Config 1: Random Forest\n",
    "    {\n",
    "        'regressor': [RandomForestRegressor(random_state=42)],\n",
    "        'regressor__n_estimators': [50, 100, 200],\n",
    "        'regressor__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    # Config 2: XGBoost (Usually the winner for tabular data)\n",
    "    {\n",
    "        'regressor': [XGBRegressor(random_state=42, objective='reg:squarederror')],\n",
    "        'regressor__n_estimators': [50, 100],\n",
    "        'regressor__learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run Grid Search (The \"Race\")\n",
    "print(\"Starting Model Training (this might take a minute)...\")\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='r2', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest Model: {grid_search.best_params_['regressor']}\")\n",
    "print(f\"Best R\u00b2 Score: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c179e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reload Data (Clean Slate)\n",
    "# Ensure I have the base data from the previous step\n",
    "df_features = df.copy().sort_values(['CATEGORY_NAME', 'SALES_DATE'])\n",
    "\n",
    "# --- A. TIME CONTEXT ---\n",
    "df_features['day'] = df_features['SALES_DATE'].dt.day\n",
    "df_features['month'] = df_features['SALES_DATE'].dt.month\n",
    "df_features['is_month_start'] = df_features['SALES_DATE'].dt.is_month_start.astype(int)\n",
    "df_features['is_month_end'] = df_features['SALES_DATE'].dt.is_month_end.astype(int) # Payday?\n",
    "\n",
    "# --- B. ROLLING WINDOWS (The \"Trend\" Detectors) ---\n",
    "# I calculate the average price over the last 7 days per category\n",
    "# Shift(1) ensures I don't cheat by using today's data to predict today\n",
    "df_features['rolling_7d_avg_price'] = df_features.groupby('CATEGORY_NAME')['AVG_PRICE'].transform(lambda x: x.shift(1).rolling(7).mean())\n",
    "df_features['rolling_7d_qty'] = df_features.groupby('CATEGORY_NAME')['QUANTITY_SOLD'].transform(lambda x: x.shift(1).rolling(7).mean())\n",
    "\n",
    "# Price Momentum: Is today cheaper than the 7-day average?\n",
    "df_features['price_momentum'] = df_features['AVG_PRICE'] / df_features['rolling_7d_avg_price']\n",
    "\n",
    "# --- C. INTERACTION TERMS ---\n",
    "# \"Weekend Rain\" might be a huge driver\n",
    "df_features['rain_weekend_interaction'] = df_features['is_weekend'] * df_features['WEATHER_CONDITION'].apply(lambda x: 1 if x == 'Rain' else 0)\n",
    "\n",
    "# --- D. CLEANUP ---\n",
    "# Rolling windows create NaNs for the first 7 days. Drop them.\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "print(f\"New Features Created! Shape: {df_features.shape}\")\n",
    "df_features[['SALES_DATE', 'CATEGORY_NAME', 'price_momentum', 'rolling_7d_qty']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. Update Feature List (X)\n",
    "# I added 'price_momentum', 'rolling_7d_qty', 'is_month_end', etc.\n",
    "features = [\n",
    "    'CATEGORY_NAME', 'WEATHER_CONDITION', \n",
    "    'AVG_PRICE', 'PRICE_RATIO', \n",
    "    'is_weekend', 'is_month_end',\n",
    "    'price_momentum', 'rolling_7d_qty', 'rain_weekend_interaction'\n",
    "]\n",
    "\n",
    "X = df_features[features]\n",
    "y = df_features['QUANTITY_SOLD']\n",
    "\n",
    "# 2. Split (Standard 80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Define Pipeline with XGBoost\n",
    "# XGBoost handles NaNs well, but I still scale numeric features\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), # Re-using your ColumnTransformer from before\n",
    "    ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Hyperparameter Tuning (Randomized Search is faster than Grid)\n",
    "param_dist = {\n",
    "    'model__n_estimators': [100, 300, 500],        # How many trees?\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],     # How fast to learn?\n",
    "    'model__max_depth': [3, 5, 7],                 # How complex?\n",
    "    'model__subsample': [0.7, 0.9, 1.0]            # Prevent overfitting\n",
    "}\n",
    "\n",
    "print(\"Starting XGBoost Tuning (This may take 2 mins)...\")\n",
    "search = RandomizedSearchCV(pipeline_xgb, param_dist, n_iter=20, cv=3, scoring='r2', verbose=1, random_state=42)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Results\n",
    "print(f\"\\nBest Params: {search.best_params_}\")\n",
    "print(f\"New R\u00b2 Score: {search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ab371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Competitor Moves (Did they change price yesterday?)\n",
    "# I compare today's competitor price to yesterday's\n",
    "df_features['comp_price_change_1d'] = df_features.groupby('CATEGORY_NAME')['AVG_COMPETITOR_PRICE'].diff()\n",
    "\n",
    "# 2. Undercut Severity (The \"Killer\" Feature)\n",
    "# Formula: (My Price - Competitor Price) / Competitor Price\n",
    "# Positive = I am expensive. Negative = I am cheap.\n",
    "df_features['undercut_pct'] = (df_features['AVG_PRICE'] - df_features['AVG_COMPETITOR_PRICE']) / df_features['AVG_COMPETITOR_PRICE']\n",
    "\n",
    "# 3. Volatility (Is the market unstable?)\n",
    "# Standard deviation of competitor price over last 7 days\n",
    "df_features['comp_volatility_7d'] = df_features.groupby('CATEGORY_NAME')['AVG_COMPETITOR_PRICE'].transform(lambda x: x.shift(1).rolling(7).std())\n",
    "\n",
    "# 4. Clean up NaNs created by 'diff' and 'rolling'\n",
    "df_final = df_features.dropna().copy()\n",
    "\n",
    "print(f\"Features Engineered! Final Data Shape: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the feature list with my new \"Hail Mary\" features\n",
    "features_v2 = features + ['comp_price_change_1d', 'undercut_pct', 'comp_volatility_7d']\n",
    "\n",
    "X = df_final[features_v2]\n",
    "y = df_final['QUANTITY_SOLD']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Retrain XGBoost (Using the best params you found earlier)\n",
    "# 'model__subsample': 0.9, 'model__n_estimators': 300, 'model__max_depth': 3, 'model__learning_rate': 0.01\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        subsample=0.9,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "score = final_pipeline.score(X_test, y_test)\n",
    "print(f\"Final Champion Score (R\u00b2): {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the \"Safe\" Feature Set (No complex rolling windows)\n",
    "# These are inputs I can easily change in the Streamlit Sidebar\n",
    "safe_features = ['CATEGORY_NAME', 'WEATHER_CONDITION', 'AVG_PRICE', 'PRICE_RATIO', 'is_weekend']\n",
    "\n",
    "X = df[safe_features]  # Uses the original dataframe, not the engineered one\n",
    "y = df['QUANTITY_SOLD']\n",
    "\n",
    "# 2. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Build the Robust Pipeline\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=300,        # High number of trees\n",
    "        learning_rate=0.05,      # Slower learning = better generalization\n",
    "        max_depth=5,             # Not too deep (prevents overfitting)\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 4. Train & Save\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "score = final_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Robust Model Trained!\")\n",
    "print(f\"Final App Score (R\u00b2): {score:.3f} (Reliable & Stable)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Get the model object from the pipeline\n",
    "# I need to dig into the pipeline steps to find the actual XGBoost model\n",
    "xgb_model = final_pipeline.named_steps['model']\n",
    "feature_names = safe_features # ['CATEGORY_NAME', 'WEATHER_CONDITION', 'AVG_PRICE', 'PRICE_RATIO', 'is_weekend']\n",
    "\n",
    "# Note: OneHotEncoding expands the features, so I need to match them up.\n",
    "# This simple plot focuses on the \"Weight\" (how many times a feature is used in a split)\n",
    "from xgboost import plot_importance\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_importance(xgb_model, max_num_features=10, height=0.5)\n",
    "plt.title(\"XGBoost Feature Importance: What Matters?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# 1. Calculate Errors\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# 2. Plot Actual vs. Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Actual Quantity Sold\")\n",
    "plt.ylabel(\"Error (Residual)\")\n",
    "plt.title(\"Residual Plot: Are errors random or systemic?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61da618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Create a synthetic dataset for ONE product category\n",
    "# I fix everything else (Clear weather, Weekday) and ONLY change Price\n",
    "prices = np.linspace(50, 2000, 100) # Test prices from $50 to $2000\n",
    "sim_data = pd.DataFrame({\n",
    "    'CATEGORY_NAME': ['eletronicos'] * 100,\n",
    "    'WEATHER_CONDITION': ['Clear'] * 100,\n",
    "    'AVG_PRICE': prices,\n",
    "    'PRICE_RATIO': prices / 250, # Assume competitor stays at $250\n",
    "    'is_weekend': [0] * 100\n",
    "})\n",
    "\n",
    "# 2. Predict Demand\n",
    "predicted_demand = final_pipeline.predict(sim_data)\n",
    "\n",
    "# 3. Plot the Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prices, predicted_demand, color='blue', linewidth=2)\n",
    "plt.xlabel(\"My Price ($)\")\n",
    "plt.ylabel(\"Predicted Quantity Sold\")\n",
    "plt.title(\"Price Elasticity Curve: Does Demand Drop as Price Rises?\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Constraints\n",
    "# I map features to directions: -1 = Downward, +1 = Upward, 0 = No constraint\n",
    "# Features order: ['CATEGORY_NAME', 'WEATHER_CONDITION', 'AVG_PRICE', 'PRICE_RATIO', 'is_weekend']\n",
    "# I only constrain 'AVG_PRICE' (Index 2) and 'PRICE_RATIO' (Index 3) to be negative (-1).\n",
    "# Note: OneHotEncoding makes index matching hard, so I apply it to the XGBoost directly inside the pipeline.\n",
    "\n",
    "# To simplify, I will use the 'interaction_constraints' or 'monotone_constraints' parameter.\n",
    "# But with pipelines, it's tricky to map indices after encoding.\n",
    "# EASIER FIX: Increase 'min_child_weight' and reduce depth to force smoothness.\n",
    "\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=200,        # Slightly fewer trees to prevent overfitting\n",
    "        learning_rate=0.03,      # Slower learning\n",
    "        max_depth=3,             # Shallow trees = Smoother curves (Crucial Fix)\n",
    "        min_child_weight=10,     # Requires more data to make a \"split\" (Removes spikes)\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Retrain\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 3. Re-run the Elasticity Check immediately\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prices = np.linspace(50, 1000, 100)\n",
    "sim_data = pd.DataFrame({\n",
    "    'CATEGORY_NAME': ['eletronicos'] * 100,\n",
    "    'WEATHER_CONDITION': ['Clear'] * 100,\n",
    "    'AVG_PRICE': prices,\n",
    "    'PRICE_RATIO': prices / 250, \n",
    "    'is_weekend': [0] * 100\n",
    "})\n",
    "\n",
    "predicted_demand = final_pipeline.predict(sim_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prices, predicted_demand, color='green', linewidth=3)\n",
    "plt.xlabel(\"My Price ($)\")\n",
    "plt.ylabel(\"Predicted Quantity Sold\")\n",
    "plt.title(\"Fixed Price Elasticity: Smoother & Safer?\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f27407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Refit on ALL data (The \"Green Line\" Configuration)\n",
    "# I use the entire dataset (X, y) instead of just X_train\n",
    "# This gives the model the maximum amount of history to learn from\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# 2. Save to disk\n",
    "# I will save it in your project root for now to keep it simple\n",
    "model_filename = 'model_pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "\n",
    "print(f\"Model frozen and saved as '{model_filename}'\")\n",
    "print(f\"   - Type: {type(final_pipeline)}\")\n",
    "print(f\"   - Steps: {list(final_pipeline.named_steps.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcde0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}